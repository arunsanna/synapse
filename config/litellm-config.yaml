# Synapse — LiteLLM Routing Configuration
# Deployed as ConfigMap in llm-infra namespace
#
# Phase 1: Ollama-only (embeddings + small LLMs)
# Phase 2: Add vLLM backends
# Phase 3: This config goes live
# Phase 4: Add TTS/STT routes

model_list:
  # === Embeddings → Ollama (CPU) ===
  - model_name: mxbai-embed-large
    litellm_params:
      model: ollama/mxbai-embed-large
      api_base: http://ollama.llm-infra.svc.cluster.local:11434

  # === Small LLMs → vLLM (GPU primary), Ollama (CPU fallback) ===
  - model_name: llama3.1-8b
    litellm_params:
      model: openai/meta-llama/Meta-Llama-3.1-8B-Instruct
      api_base: http://vllm-inference.llm-infra.svc.cluster.local:8001/v1
      order: 1 # GPU primary

  - model_name: llama3.1-8b
    litellm_params:
      model: ollama/llama3.1:8b
      api_base: http://ollama.llm-infra.svc.cluster.local:11434
      order: 2 # CPU fallback

  # === Large LLMs → vLLM (GPU only) ===
  - model_name: llama3.1-70b
    litellm_params:
      model: openai/meta-llama/Meta-Llama-3.1-70B-Instruct
      api_base: http://vllm-inference.llm-infra.svc.cluster.local:8001/v1

  # === STT → Speaches (faster-whisper, NOT vLLM Whisper) ===
  - model_name: whisper-large-v3
    litellm_params:
      model: openai/whisper-large-v3
      api_base: http://speaches-stt.llm-infra.svc.cluster.local:8002/v1

  # === TTS → Piper (CPU, standard quality) ===
  - model_name: piper-tts
    litellm_params:
      model: tts-1
      api_base: http://piper-tts.llm-infra.svc.cluster.local:8003/v1

router_settings:
  routing_strategy: simple-shuffle
  num_retries: 2
  timeout: 600
  enable_pre_call_checks: true
